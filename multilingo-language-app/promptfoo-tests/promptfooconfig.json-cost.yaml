# promptfooconfig.yaml
description: "Comparing Gemini 1.5 Flash and Pro for feedback generation cost and quality."

providers:
  - id: google:gemini-2.0-flash # Our current feedback model (generally cheaper)
  - id: google:gemini-2.0-pro-exp-02-05   # A more powerful alternative (generally more expensive)

prompts:
  # Prompt for generating student feedback
  - id: feedback-prompt
    prompt: |
      ## System
      You are a friendly language tutor.
      ## User
      Based on the following grading in {{language}}, provide constructive feedback to the student in English so they can improve in learning the language.
      If the mark is 10, just say the user did a great work. Do not use markdown for the feedback message.
      The feedback should be concise and direct.
      
      Grading:
      {
        'mark': {{mark}},
        'mistakes': {{mistakes}}
      }
      
      Return JSON with a single key 'feedback_message' containing the message.
      Feedback:

tests:

  # Test Case 1: Feedback for a French answer with a minor mistake - Check quality and latency
  - description: 'Feedback for French answer (mark 7, gender mistake)'
    vars:
      language: "French"
      mark: 7
      mistakes: "['gender agreement']"
      prompt: feedback-prompt
    assert:
      - type: is-json
        metric: "Format"
      - type: llm-rubric # ASSERTION: Use another LLM to grade the feedback quality
        value: "Feedback is constructive, addresses the 'gender agreement' mistake, and is encouraging."
        provider: openai:gpt-4o # Using GPT-4o as a robust grader for subjective quality
        metric: "Feedback Quality"
        weight: 2 # Give more weight to qualitative feedback
      - type: latency # ASSERTION: Ensure the response is fast
        threshold: 1500 # Max 1.5 seconds
        metric: "Performance"

  # Test Case 2: Feedback for a low score with multiple mistakes - Check conciseness and relevant terms
  - description: 'Feedback for German answer (mark 3, multiple mistakes)'
    vars:
      language: "German"
      mark: 3
      mistakes: "['verb conjugation', 'vocabulary', 'sentence structure']"
      prompt: feedback-prompt
    assert:
      - type: is-json
        metric: "Format"
      - type: contains-all # ASSERTION: Ensure all identified mistakes are mentioned
        value: ["conjugation", "vocabulary", "sentence structure"]
        metric: "Content Accuracy"
      - type: javascript # ASSERTION: Ensure the feedback is not excessively long
        value: JSON.parse(output).feedback_message.length < 300
        metric: "Conciseness"
      - type: cost
        threshold: 0.00005 # Again, Pro will likely fail this cost assertion
        metric: "Cost"
